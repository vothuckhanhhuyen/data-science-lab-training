{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RqEwB3ii0LL3AFyGldCTAr60hJTlJCto","authorship_tag":"ABX9TyNcHkqLwAqMMn7BmPT0UVl1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ADNucUocrZLu"},"source":["**1. Tensorflow**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkHZMJHaFFDv","executionInfo":{"status":"ok","timestamp":1621005163495,"user_tz":-420,"elapsed":2682,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"7e30c2f6-6214-4148-a0f4-610b9a988168"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hm18ZdFVX_FT","executionInfo":{"status":"ok","timestamp":1621005163498,"user_tz":-420,"elapsed":2674,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["x1 = tf.constant(5)\n","x2 = tf.constant(6)\n","\n","result = tf.multiply(x1, x2)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4HxSsyzYEqA","executionInfo":{"status":"ok","timestamp":1621005163499,"user_tz":-420,"elapsed":2670,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"05a8a82e-7c74-47f7-ad47-30beeebb64b8"},"source":["print(result)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensor(\"Mul:0\", shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrmP8QOOYIMg","executionInfo":{"status":"ok","timestamp":1621005163500,"user_tz":-420,"elapsed":2660,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"c254d00c-e769-445b-b975-50d127915f56"},"source":["with tf.Session() as sess:\n","  output = sess.run(result)\n","  print(output)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rN2yFyeQFuu2"},"source":["**2. Triá»ƒn khai MLP**"]},{"cell_type":"code","metadata":{"id":"wzFeB3hse8En","executionInfo":{"status":"ok","timestamp":1621005163502,"user_tz":-420,"elapsed":2655,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["import numpy as np\n","import random"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYff0somJB4L","executionInfo":{"status":"ok","timestamp":1621005163503,"user_tz":-420,"elapsed":2651,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["tf.reset_default_graph()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_r8QLdB9Ha8L"},"source":["**a. class MLP**"]},{"cell_type":"code","metadata":{"id":"4cvZ8mOpGHU0","executionInfo":{"status":"ok","timestamp":1621005163503,"user_tz":-420,"elapsed":2647,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["class MLP:\n","  def __init__(self, vocab_size, hidden_size):\n","    self._vocab_size = vocab_size\n","    self._hidden_size = hidden_size\n","\n","  def build_graph(self):\n","    self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n","    self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\n","\n","    weights_1 = tf.get_variable(\n","        name='weights_input_hidden',\n","        shape=(self._vocab_size, self._hidden_size),\n","        initializer=tf.random_normal_initializer(seed=2021)\n","    )\n","    biases_1 = tf.get_variable(\n","        name='biases_input_hidden',\n","        shape=(self._hidden_size),\n","        initializer=tf.random_normal_initializer(seed=2021)\n","    )\n","    weights_2 = tf.get_variable(\n","        name='weights_hidden_output',\n","        shape=(self._hidden_size, NUM_CLASSES),\n","        initializer=tf.random_normal_initializer(seed=2021)\n","    )\n","    biases_2 = tf.get_variable(\n","        name='biases_hidden_output',\n","        shape=(NUM_CLASSES),\n","        initializer=tf.random_normal_initializer(seed=2021)\n","    )\n","\n","    hidden = tf.matmul(self._X, weights_1) + biases_1\n","    hidden = tf.sigmoid(hidden)\n","    logits = tf.matmul(hidden, weights_2) + biases_2\n","\n","    labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n","    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    predicted_labels = tf.argmax(probs, axis=1)\n","    predicted_labels = tf.squeeze(predicted_labels)\n","\n","    return predicted_labels, loss\n","\n","  def trainer(self, loss, learning_rate):\n","    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","    return train_op"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uxx4kXtQHfOy"},"source":["**b. class DataReader**"]},{"cell_type":"code","metadata":{"id":"6cnae7kLdUa9","executionInfo":{"status":"ok","timestamp":1621005163503,"user_tz":-420,"elapsed":2643,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["class DataReader:\n","  def __init__(self, data_path, batch_size, vocab_size):\n","    self._batch_size = batch_size\n","    with open(data_path) as f:\n","      d_lines = f.read().splitlines()\n","\n","    self._data = []\n","    self._labels = []\n","    for data_id, line in enumerate(d_lines):\n","      vector = [0.0 for _ in range(vocab_size)]\n","      features = line.split('<fff>')\n","      label, doc_id = int(features[0]), int(features[1])\n","      tokens = features[2].split()\n","\n","      for token in tokens:\n","        index, value = int(token.split(':')[0]), float(token.split(':')[1])\n","        vector[index] = value\n","        \n","      self._data.append(vector)\n","      self._labels.append(label)\n","\n","    self._data = np.array(self._data)\n","    self._labels = np.array(self._labels)\n","    self._num_epoch = 0\n","    self._batch_id = 0\n","\n","  def next_batch(self):\n","    start = self._batch_id * self._batch_size\n","    end = start + self._batch_size\n","    self._batch_id += 1\n","\n","    if end + self._batch_size > len(self._data):\n","      end = len(self._data)\n","      self._num_epoch += 1\n","      self._batch_id = 0\n","      indices = list(range(len(self._data)))\n","      random.seed(2021)\n","      random.shuffle(indices)\n","      self._data, self._labels = self._data[indices], self._labels[indices]\n","\n","    return self._data[start:end], self._labels[start:end]"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R94NoWd7Hm5J"},"source":["**c. function:**\n","+ load_dataset\n","+ save_parameters\n","+ restore_parameters"]},{"cell_type":"code","metadata":{"id":"PouAXmBzPazq","executionInfo":{"status":"ok","timestamp":1621005163504,"user_tz":-420,"elapsed":2639,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["# load_dataset()\n","\n","def load_dataset():\n","  train_data_reader = DataReader(\n","      data_path='/content/drive/MyDrive/Colab Notebooks/DSLAB TRAINING/SESSION 3/data/20news-train-tfidf.txt',\n","      batch_size=50,\n","      vocab_size=vocab_size\n","  )\n","  test_data_reader = DataReader(\n","      data_path='/content/drive/MyDrive/Colab Notebooks/DSLAB TRAINING/SESSION 3/data/20news-test-tfidf.txt',\n","      batch_size=50,\n","      vocab_size=vocab_size\n","  )\n","  return train_data_reader, test_data_reader"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jLpSkgu7zpg","executionInfo":{"status":"ok","timestamp":1621005163504,"user_tz":-420,"elapsed":2635,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["# save_parameters\n","\n","def save_parameters(name, value, epoch):\n","  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","  if len(value.shape) == 1: # is a list\n","    string_form = ','.join([str(number) for number in value])\n","  else:\n","    string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n","  with open('/content/drive/MyDrive/Colab Notebooks/DSLAB TRAINING/SESSION 3/data/saved-paras/' + filename, 'w') as f:\n","    f.write(string_form)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCFMLa1V_YW3","executionInfo":{"status":"ok","timestamp":1621005163505,"user_tz":-420,"elapsed":2632,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}}},"source":["# restore_parameters\n","\n","def restore_parameters(name, epoch):\n","  filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","  with open('/content/drive/MyDrive/Colab Notebooks/DSLAB TRAINING/SESSION 3/data/saved-paras/' + filename) as f:\n","    lines = f.read().splitlines()\n","  if len(lines) == 1: # is a vector\n","    value = [float(number) for number in lines[0].split(',')]\n","  else: # is a matrix\n","    value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n","  return value"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oEhj1JfMH5vR"},"source":["**d. main**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Sao2QpZK0vj","executionInfo":{"status":"ok","timestamp":1621005191599,"user_tz":-420,"elapsed":30721,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"ce3324a9-c291-4c51-9782-5b2a6bee2302"},"source":["# create a computation graph\n","\n","with open('/content/drive/MyDrive/Colab Notebooks/DSLAB TRAINING/SESSION 3/data/words_idfs.txt') as f:\n","  vocab_size = len(f.read().splitlines())\n","  \n","NUM_CLASSES = 20\n","\n","mlp = MLP(\n","    vocab_size=vocab_size, \n","    hidden_size=50\n",")\n","\n","predicted_labels, loss = mlp.build_graph()\n","train_op = mlp.trainer(loss=loss, learning_rate=0.1)\n","\n","train_data_reader, test_data_reader = load_dataset()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-VOHruAmKiF","executionInfo":{"status":"ok","timestamp":1621005771513,"user_tz":-420,"elapsed":39889,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"372b3b85-3163-450b-aba8-bca288c06a89"},"source":["# open a session to run\n","\n","with tf.Session() as sess:\n","  step, MAX_STEP = 0, 1000\n","\n","  sess.run(tf.global_variables_initializer())\n","  while step < MAX_STEP:\n","    train_data, train_labels = train_data_reader.next_batch()\n","    plabels_eval, loss_eval, _ = sess.run(\n","        [predicted_labels, loss, train_op],\n","        feed_dict = {\n","            mlp._X: train_data,\n","            mlp._real_Y: train_labels\n","        }\n","    )\n","    step += 1\n","    print('step: {}, loss: {}'.format(step, loss_eval))\n","\n","    trainable_variables = tf.trainable_variables()\n","    for variable in trainable_variables:\n","      save_parameters(\n","          name=variable.name,\n","          value=variable.eval(),\n","          epoch=train_data_reader._num_epoch\n","      )"],"execution_count":13,"outputs":[{"output_type":"stream","text":["step: 1, loss: 12.036225318908691\n","step: 2, loss: 2.477519989013672\n","step: 3, loss: 0.0009005915489979088\n","step: 4, loss: 1.212670213135425e-05\n","step: 5, loss: 1.0490408186569766e-07\n","step: 6, loss: 0.0\n","step: 7, loss: 17.395544052124023\n","step: 8, loss: 25.100133895874023\n","step: 9, loss: 21.139331817626953\n","step: 10, loss: 15.55300521850586\n","step: 11, loss: 9.098770141601562\n","step: 12, loss: 3.6952903270721436\n","step: 13, loss: 0.9884459972381592\n","step: 14, loss: 0.060531944036483765\n","step: 15, loss: 21.10125732421875\n","step: 16, loss: 24.458280563354492\n","step: 17, loss: 22.112314224243164\n","step: 18, loss: 18.431842803955078\n","step: 19, loss: 14.820002555847168\n","step: 20, loss: 10.447005271911621\n","step: 21, loss: 6.201825141906738\n","step: 22, loss: 2.4968008995056152\n","step: 23, loss: 15.853484153747559\n","step: 24, loss: 16.919397354125977\n","step: 25, loss: 15.861337661743164\n","step: 26, loss: 13.748368263244629\n","step: 27, loss: 11.361886978149414\n","step: 28, loss: 9.32450008392334\n","step: 29, loss: 6.441883563995361\n","step: 30, loss: 5.4386162757873535\n","step: 31, loss: 16.01528549194336\n","step: 32, loss: 14.379433631896973\n","step: 33, loss: 13.176689147949219\n","step: 34, loss: 11.576562881469727\n","step: 35, loss: 9.624707221984863\n","step: 36, loss: 8.484454154968262\n","step: 37, loss: 6.721511363983154\n","step: 38, loss: 7.506026744842529\n","step: 39, loss: 9.99383544921875\n","step: 40, loss: 7.924896240234375\n","step: 41, loss: 5.767838954925537\n","step: 42, loss: 4.642287731170654\n","step: 43, loss: 3.128953218460083\n","step: 44, loss: 1.9378987550735474\n","step: 45, loss: 1.5489519834518433\n","step: 46, loss: 4.422839164733887\n","step: 47, loss: 8.232155799865723\n","step: 48, loss: 8.219164848327637\n","step: 49, loss: 7.52487850189209\n","step: 50, loss: 6.509955406188965\n","step: 51, loss: 5.903184413909912\n","step: 52, loss: 5.043768882751465\n","step: 53, loss: 4.173044681549072\n","step: 54, loss: 9.928016662597656\n","step: 55, loss: 10.568381309509277\n","step: 56, loss: 9.746400833129883\n","step: 57, loss: 8.011825561523438\n","step: 58, loss: 6.638452053070068\n","step: 59, loss: 5.985235214233398\n","step: 60, loss: 5.019514560699463\n","step: 61, loss: 4.151669979095459\n","step: 62, loss: 3.7963132858276367\n","step: 63, loss: 3.6196389198303223\n","step: 64, loss: 3.267224073410034\n","step: 65, loss: 3.017604112625122\n","step: 66, loss: 2.72867488861084\n","step: 67, loss: 2.608772039413452\n","step: 68, loss: 2.2932822704315186\n","step: 69, loss: 2.0568063259124756\n","step: 70, loss: 7.644775390625\n","step: 71, loss: 8.139880180358887\n","step: 72, loss: 8.191669464111328\n","step: 73, loss: 7.736568450927734\n","step: 74, loss: 7.4232330322265625\n","step: 75, loss: 7.091252326965332\n","step: 76, loss: 6.9402971267700195\n","step: 77, loss: 6.505837917327881\n","step: 78, loss: 8.260019302368164\n","step: 79, loss: 7.889408588409424\n","step: 80, loss: 7.692651271820068\n","step: 81, loss: 7.387941360473633\n","step: 82, loss: 7.269172191619873\n","step: 83, loss: 6.835662841796875\n","step: 84, loss: 6.520843029022217\n","step: 85, loss: 6.377313613891602\n","step: 86, loss: 7.23564338684082\n","step: 87, loss: 7.031421661376953\n","step: 88, loss: 6.844564914703369\n","step: 89, loss: 6.669028282165527\n","step: 90, loss: 6.391623497009277\n","step: 91, loss: 6.16599178314209\n","step: 92, loss: 5.896223545074463\n","step: 93, loss: 5.619499683380127\n","step: 94, loss: 10.623893737792969\n","step: 95, loss: 10.34779167175293\n","step: 96, loss: 10.046629905700684\n","step: 97, loss: 9.61099910736084\n","step: 98, loss: 9.379085540771484\n","step: 99, loss: 9.227767944335938\n","step: 100, loss: 9.189071655273438\n","step: 101, loss: 8.758021354675293\n","step: 102, loss: 9.798644065856934\n","step: 103, loss: 9.662740707397461\n","step: 104, loss: 9.671692848205566\n","step: 105, loss: 9.480820655822754\n","step: 106, loss: 8.9733247756958\n","step: 107, loss: 8.652565002441406\n","step: 108, loss: 8.504093170166016\n","step: 109, loss: 8.208992958068848\n","step: 110, loss: 8.248377799987793\n","step: 111, loss: 8.084491729736328\n","step: 112, loss: 7.875616550445557\n","step: 113, loss: 7.618342399597168\n","step: 114, loss: 7.399133205413818\n","step: 115, loss: 7.152212619781494\n","step: 116, loss: 6.88250732421875\n","step: 117, loss: 6.191530704498291\n","step: 118, loss: 5.411104202270508\n","step: 119, loss: 5.075531959533691\n","step: 120, loss: 4.7871479988098145\n","step: 121, loss: 4.55235481262207\n","step: 122, loss: 4.294675827026367\n","step: 123, loss: 3.926039934158325\n","step: 124, loss: 3.586956262588501\n","step: 125, loss: 4.8844122886657715\n","step: 126, loss: 7.274083137512207\n","step: 127, loss: 7.0757269859313965\n","step: 128, loss: 6.855058193206787\n","step: 129, loss: 6.634847640991211\n","step: 130, loss: 6.355490684509277\n","step: 131, loss: 6.002030849456787\n","step: 132, loss: 5.490891933441162\n","step: 133, loss: 4.0304155349731445\n","step: 134, loss: 3.8452343940734863\n","step: 135, loss: 3.4614529609680176\n","step: 136, loss: 3.082085609436035\n","step: 137, loss: 2.734717607498169\n","step: 138, loss: 2.0760791301727295\n","step: 139, loss: 1.4728460311889648\n","step: 140, loss: 6.382645130157471\n","step: 141, loss: 10.259803771972656\n","step: 142, loss: 10.138051986694336\n","step: 143, loss: 9.680791854858398\n","step: 144, loss: 9.379558563232422\n","step: 145, loss: 8.951445579528809\n","step: 146, loss: 10.588359832763672\n","step: 147, loss: 14.169438362121582\n","step: 148, loss: 12.705911636352539\n","step: 149, loss: 11.739880561828613\n","step: 150, loss: 3.2504208087921143\n","step: 151, loss: 3.2162649631500244\n","step: 152, loss: 2.778747320175171\n","step: 153, loss: 3.0614449977874756\n","step: 154, loss: 2.8166086673736572\n","step: 155, loss: 2.914489507675171\n","step: 156, loss: 2.6253738403320312\n","step: 157, loss: 2.3711984157562256\n","step: 158, loss: 2.638350009918213\n","step: 159, loss: 2.9578332901000977\n","step: 160, loss: 2.6353893280029297\n","step: 161, loss: 2.572349786758423\n","step: 162, loss: 2.5857932567596436\n","step: 163, loss: 2.447983741760254\n","step: 164, loss: 2.605837106704712\n","step: 165, loss: 2.3996033668518066\n","step: 166, loss: 2.1834490299224854\n","step: 167, loss: 2.4479212760925293\n","step: 168, loss: 2.4765262603759766\n","step: 169, loss: 2.8338091373443604\n","step: 170, loss: 2.2941393852233887\n","step: 171, loss: 2.571143865585327\n","step: 172, loss: 1.9853065013885498\n","step: 173, loss: 2.4844934940338135\n","step: 174, loss: 2.23539662361145\n","step: 175, loss: 2.2072925567626953\n","step: 176, loss: 2.182185411453247\n","step: 177, loss: 2.018594980239868\n","step: 178, loss: 2.0501961708068848\n","step: 179, loss: 1.9441733360290527\n","step: 180, loss: 2.1634480953216553\n","step: 181, loss: 1.955834984779358\n","step: 182, loss: 2.017285108566284\n","step: 183, loss: 1.4805350303649902\n","step: 184, loss: 1.8443626165390015\n","step: 185, loss: 2.1466054916381836\n","step: 186, loss: 1.8159551620483398\n","step: 187, loss: 1.5769866704940796\n","step: 188, loss: 1.969735860824585\n","step: 189, loss: 2.049203872680664\n","step: 190, loss: 1.51694917678833\n","step: 191, loss: 1.9591875076293945\n","step: 192, loss: 1.7829079627990723\n","step: 193, loss: 1.7752869129180908\n","step: 194, loss: 1.8262524604797363\n","step: 195, loss: 1.945870041847229\n","step: 196, loss: 1.846328616142273\n","step: 197, loss: 1.5020326375961304\n","step: 198, loss: 1.671718955039978\n","step: 199, loss: 1.8637064695358276\n","step: 200, loss: 1.4168394804000854\n","step: 201, loss: 1.3696568012237549\n","step: 202, loss: 1.5160858631134033\n","step: 203, loss: 1.5998073816299438\n","step: 204, loss: 1.247773289680481\n","step: 205, loss: 1.2137174606323242\n","step: 206, loss: 1.5264419317245483\n","step: 207, loss: 1.5138826370239258\n","step: 208, loss: 1.1789028644561768\n","step: 209, loss: 1.6614896059036255\n","step: 210, loss: 0.8373441100120544\n","step: 211, loss: 1.0742197036743164\n","step: 212, loss: 1.3981337547302246\n","step: 213, loss: 1.5030348300933838\n","step: 214, loss: 1.1819628477096558\n","step: 215, loss: 1.3505061864852905\n","step: 216, loss: 1.133826732635498\n","step: 217, loss: 1.2231899499893188\n","step: 218, loss: 1.0245254039764404\n","step: 219, loss: 0.9689262509346008\n","step: 220, loss: 0.9898964762687683\n","step: 221, loss: 1.1051232814788818\n","step: 222, loss: 0.8535780310630798\n","step: 223, loss: 0.913117527961731\n","step: 224, loss: 1.447153925895691\n","step: 225, loss: 1.0425208806991577\n","step: 226, loss: 0.9471141695976257\n","step: 227, loss: 0.9980352520942688\n","step: 228, loss: 0.6937212944030762\n","step: 229, loss: 1.0080207586288452\n","step: 230, loss: 0.6884790658950806\n","step: 231, loss: 1.0961841344833374\n","step: 232, loss: 0.9566845893859863\n","step: 233, loss: 0.7174292802810669\n","step: 234, loss: 1.2882293462753296\n","step: 235, loss: 0.797056257724762\n","step: 236, loss: 1.062087059020996\n","step: 237, loss: 0.8319032788276672\n","step: 238, loss: 0.89052414894104\n","step: 239, loss: 0.8381908535957336\n","step: 240, loss: 0.6608238816261292\n","step: 241, loss: 0.7642576098442078\n","step: 242, loss: 0.7188992500305176\n","step: 243, loss: 1.1030042171478271\n","step: 244, loss: 0.7179034352302551\n","step: 245, loss: 1.179612159729004\n","step: 246, loss: 0.9314073324203491\n","step: 247, loss: 0.6648164987564087\n","step: 248, loss: 1.1746199131011963\n","step: 249, loss: 0.7546068429946899\n","step: 250, loss: 0.36372485756874084\n","step: 251, loss: 0.5089257955551147\n","step: 252, loss: 0.5402891635894775\n","step: 253, loss: 0.6630650162696838\n","step: 254, loss: 0.6016770005226135\n","step: 255, loss: 0.7775720357894897\n","step: 256, loss: 0.4231717586517334\n","step: 257, loss: 0.6639894247055054\n","step: 258, loss: 0.5605615377426147\n","step: 259, loss: 0.845211923122406\n","step: 260, loss: 0.6042625308036804\n","step: 261, loss: 0.32976439595222473\n","step: 262, loss: 0.5508626699447632\n","step: 263, loss: 0.562243640422821\n","step: 264, loss: 0.815299391746521\n","step: 265, loss: 0.3221122622489929\n","step: 266, loss: 0.36169832944869995\n","step: 267, loss: 0.683133602142334\n","step: 268, loss: 0.46270450949668884\n","step: 269, loss: 0.5204268097877502\n","step: 270, loss: 0.4811079502105713\n","step: 271, loss: 0.3231468200683594\n","step: 272, loss: 0.5444636344909668\n","step: 273, loss: 0.8833844065666199\n","step: 274, loss: 0.628669798374176\n","step: 275, loss: 0.6189618110656738\n","step: 276, loss: 0.5718284249305725\n","step: 277, loss: 0.3517841696739197\n","step: 278, loss: 0.8949832320213318\n","step: 279, loss: 0.28561997413635254\n","step: 280, loss: 0.8420363068580627\n","step: 281, loss: 0.6220677495002747\n","step: 282, loss: 0.33506539463996887\n","step: 283, loss: 0.42647793889045715\n","step: 284, loss: 0.5281926989555359\n","step: 285, loss: 0.3407853841781616\n","step: 286, loss: 0.6142289042472839\n","step: 287, loss: 0.7674928307533264\n","step: 288, loss: 0.7299301624298096\n","step: 289, loss: 0.707193911075592\n","step: 290, loss: 0.29025259613990784\n","step: 291, loss: 0.26109689474105835\n","step: 292, loss: 0.4941446781158447\n","step: 293, loss: 0.5071046352386475\n","step: 294, loss: 0.33529701828956604\n","step: 295, loss: 0.5644561648368835\n","step: 296, loss: 0.59530109167099\n","step: 297, loss: 0.41283339262008667\n","step: 298, loss: 0.5865045785903931\n","step: 299, loss: 0.4627798795700073\n","step: 300, loss: 0.1514805406332016\n","step: 301, loss: 0.10396275669336319\n","step: 302, loss: 0.1838633120059967\n","step: 303, loss: 0.144121453166008\n","step: 304, loss: 0.1391097754240036\n","step: 305, loss: 0.2567996084690094\n","step: 306, loss: 0.273723840713501\n","step: 307, loss: 0.24823439121246338\n","step: 308, loss: 0.15853798389434814\n","step: 309, loss: 0.3191431760787964\n","step: 310, loss: 0.13327589631080627\n","step: 311, loss: 0.16494911909103394\n","step: 312, loss: 0.11108633130788803\n","step: 313, loss: 0.1738835722208023\n","step: 314, loss: 0.1904938668012619\n","step: 315, loss: 0.30486181378364563\n","step: 316, loss: 0.13609670102596283\n","step: 317, loss: 0.26844680309295654\n","step: 318, loss: 0.20527011156082153\n","step: 319, loss: 0.0984923392534256\n","step: 320, loss: 0.10022785514593124\n","step: 321, loss: 0.3452626168727875\n","step: 322, loss: 0.2115747481584549\n","step: 323, loss: 0.28795069456100464\n","step: 324, loss: 0.19203811883926392\n","step: 325, loss: 0.08098698407411575\n","step: 326, loss: 0.029521549120545387\n","step: 327, loss: 0.16016271710395813\n","step: 328, loss: 0.18826857209205627\n","step: 329, loss: 0.13592980802059174\n","step: 330, loss: 0.11720404028892517\n","step: 331, loss: 0.21402087807655334\n","step: 332, loss: 0.0495089516043663\n","step: 333, loss: 0.2827075123786926\n","step: 334, loss: 0.23205673694610596\n","step: 335, loss: 0.16423676908016205\n","step: 336, loss: 0.2476193755865097\n","step: 337, loss: 0.05097818002104759\n","step: 338, loss: 0.04179643094539642\n","step: 339, loss: 0.11302722990512848\n","step: 340, loss: 0.1283305287361145\n","step: 341, loss: 0.14577741920948029\n","step: 342, loss: 0.09252697229385376\n","step: 343, loss: 0.11805332452058792\n","step: 344, loss: 0.16063192486763\n","step: 345, loss: 0.3572755455970764\n","step: 346, loss: 0.16972970962524414\n","step: 347, loss: 0.06531646102666855\n","step: 348, loss: 0.13116306066513062\n","step: 349, loss: 0.09499478340148926\n","step: 350, loss: 0.21436001360416412\n","step: 351, loss: 0.01616835780441761\n","step: 352, loss: 0.1677984744310379\n","step: 353, loss: 0.1979794055223465\n","step: 354, loss: 0.017448551952838898\n","step: 355, loss: 0.15133512020111084\n","step: 356, loss: 0.18551181256771088\n","step: 357, loss: 0.07293862849473953\n","step: 358, loss: 0.2196386307477951\n","step: 359, loss: 0.1433791220188141\n","step: 360, loss: 0.2618243098258972\n","step: 361, loss: 0.022963538765907288\n","step: 362, loss: 0.06801390647888184\n","step: 363, loss: 0.1848132163286209\n","step: 364, loss: 0.23939938843250275\n","step: 365, loss: 0.11840210109949112\n","step: 366, loss: 0.13563090562820435\n","step: 367, loss: 0.16339805722236633\n","step: 368, loss: 0.16224302351474762\n","step: 369, loss: 0.44183310866355896\n","step: 370, loss: 0.22100862860679626\n","step: 371, loss: 0.17292998731136322\n","step: 372, loss: 0.1341497153043747\n","step: 373, loss: 0.03791522979736328\n","step: 374, loss: 0.29084742069244385\n","step: 375, loss: 0.29936888813972473\n","step: 376, loss: 0.16693946719169617\n","step: 377, loss: 0.2510077953338623\n","step: 378, loss: 0.15338653326034546\n","step: 379, loss: 0.33550941944122314\n","step: 380, loss: 0.1359870880842209\n","step: 381, loss: 0.21753528714179993\n","step: 382, loss: 0.13773100078105927\n","step: 383, loss: 0.16849693655967712\n","step: 384, loss: 0.17472843825817108\n","step: 385, loss: 0.23280014097690582\n","step: 386, loss: 0.22522102296352386\n","step: 387, loss: 0.3724227547645569\n","step: 388, loss: 0.0507802851498127\n","step: 389, loss: 0.22000116109848022\n","step: 390, loss: 0.16686120629310608\n","step: 391, loss: 0.02672353759407997\n","step: 392, loss: 0.10825994610786438\n","step: 393, loss: 0.12032454460859299\n","step: 394, loss: 0.05898112431168556\n","step: 395, loss: 0.06527920067310333\n","step: 396, loss: 0.2613498568534851\n","step: 397, loss: 0.21680864691734314\n","step: 398, loss: 0.28004512190818787\n","step: 399, loss: 0.199817955493927\n","step: 400, loss: 0.19595806300640106\n","step: 401, loss: 0.1516091376543045\n","step: 402, loss: 0.06891561299562454\n","step: 403, loss: 0.16039761900901794\n","step: 404, loss: 0.0434972420334816\n","step: 405, loss: 0.2833377718925476\n","step: 406, loss: 0.18956205248832703\n","step: 407, loss: 0.10456139594316483\n","step: 408, loss: 0.1312108039855957\n","step: 409, loss: 0.3400694727897644\n","step: 410, loss: 0.24439020454883575\n","step: 411, loss: 0.18606628477573395\n","step: 412, loss: 0.17961636185646057\n","step: 413, loss: 0.13880924880504608\n","step: 414, loss: 0.2588580250740051\n","step: 415, loss: 0.01573389582335949\n","step: 416, loss: 0.13761214911937714\n","step: 417, loss: 0.15697582066059113\n","step: 418, loss: 0.08438438177108765\n","step: 419, loss: 0.21511921286582947\n","step: 420, loss: 0.15321575105190277\n","step: 421, loss: 0.20984579622745514\n","step: 422, loss: 0.12965244054794312\n","step: 423, loss: 0.134801983833313\n","step: 424, loss: 0.22966216504573822\n","step: 425, loss: 0.150440514087677\n","step: 426, loss: 0.06820827722549438\n","step: 427, loss: 0.16399016976356506\n","step: 428, loss: 0.21384777128696442\n","step: 429, loss: 0.19313101470470428\n","step: 430, loss: 0.15520891547203064\n","step: 431, loss: 0.3483494818210602\n","step: 432, loss: 0.14954447746276855\n","step: 433, loss: 0.23567315936088562\n","step: 434, loss: 0.48296523094177246\n","step: 435, loss: 0.08055663108825684\n","step: 436, loss: 0.30042946338653564\n","step: 437, loss: 0.18323126435279846\n","step: 438, loss: 0.061635904014110565\n","step: 439, loss: 0.11790693551301956\n","step: 440, loss: 0.12640787661075592\n","step: 441, loss: 0.19722269475460052\n","step: 442, loss: 0.07575509697198868\n","step: 443, loss: 0.20661842823028564\n","step: 444, loss: 0.023555224761366844\n","step: 445, loss: 0.22158758342266083\n","step: 446, loss: 0.35835984349250793\n","step: 447, loss: 0.30427709221839905\n","step: 448, loss: 0.1328158974647522\n","step: 449, loss: 0.14450901746749878\n","step: 450, loss: 0.016401777043938637\n","step: 451, loss: 0.03594129905104637\n","step: 452, loss: 0.02418125607073307\n","step: 453, loss: 0.00585568742826581\n","step: 454, loss: 0.017179390415549278\n","step: 455, loss: 0.021069763228297234\n","step: 456, loss: 0.01020857784897089\n","step: 457, loss: 0.011845051310956478\n","step: 458, loss: 0.009616377763450146\n","step: 459, loss: 0.011026255786418915\n","step: 460, loss: 0.017030231654644012\n","step: 461, loss: 0.012802381068468094\n","step: 462, loss: 0.00285321450792253\n","step: 463, loss: 0.034574590623378754\n","step: 464, loss: 0.007803902495652437\n","step: 465, loss: 0.029221098870038986\n","step: 466, loss: 0.044001203030347824\n","step: 467, loss: 0.06526705622673035\n","step: 468, loss: 0.1131611242890358\n","step: 469, loss: 0.00515170581638813\n","step: 470, loss: 0.023592861369252205\n","step: 471, loss: 0.08322091400623322\n","step: 472, loss: 0.20232602953910828\n","step: 473, loss: 0.027779296040534973\n","step: 474, loss: 0.023403814062476158\n","step: 475, loss: 0.008269179612398148\n","step: 476, loss: 0.008026368916034698\n","step: 477, loss: 0.009348759427666664\n","step: 478, loss: 0.013105630874633789\n","step: 479, loss: 0.07357782125473022\n","step: 480, loss: 0.0899365171790123\n","step: 481, loss: 0.052055977284908295\n","step: 482, loss: 0.04195721074938774\n","step: 483, loss: 0.04104979336261749\n","step: 484, loss: 0.02532578632235527\n","step: 485, loss: 0.029488539323210716\n","step: 486, loss: 0.0035338904708623886\n","step: 487, loss: 0.02209191955626011\n","step: 488, loss: 0.1917787343263626\n","step: 489, loss: 0.04571077972650528\n","step: 490, loss: 0.019536437466740608\n","step: 491, loss: 0.02767716720700264\n","step: 492, loss: 0.0509442538022995\n","step: 493, loss: 0.006765621714293957\n","step: 494, loss: 0.04993008077144623\n","step: 495, loss: 0.006514668930321932\n","step: 496, loss: 0.014676075428724289\n","step: 497, loss: 0.053440943360328674\n","step: 498, loss: 0.08630974590778351\n","step: 499, loss: 0.007632915396243334\n","step: 500, loss: 0.1001366674900055\n","step: 501, loss: 0.024746187031269073\n","step: 502, loss: 0.027726665139198303\n","step: 503, loss: 0.017858169972896576\n","step: 504, loss: 0.018205618485808372\n","step: 505, loss: 0.009234849363565445\n","step: 506, loss: 0.05315703526139259\n","step: 507, loss: 0.018003379926085472\n","step: 508, loss: 0.01553785428404808\n","step: 509, loss: 0.03042014129459858\n","step: 510, loss: 0.0032845535315573215\n","step: 511, loss: 0.042945727705955505\n","step: 512, loss: 0.006325877737253904\n","step: 513, loss: 0.002353585558012128\n","step: 514, loss: 0.009157286025583744\n","step: 515, loss: 0.12944279611110687\n","step: 516, loss: 0.009366724640130997\n","step: 517, loss: 0.009531349875032902\n","step: 518, loss: 0.028092341497540474\n","step: 519, loss: 0.05507821962237358\n","step: 520, loss: 0.00518406555056572\n","step: 521, loss: 0.056047338992357254\n","step: 522, loss: 0.030055295675992966\n","step: 523, loss: 0.006460213102400303\n","step: 524, loss: 0.016604740172624588\n","step: 525, loss: 0.021065229550004005\n","step: 526, loss: 0.005330108571797609\n","step: 527, loss: 0.015243161469697952\n","step: 528, loss: 0.023315291851758957\n","step: 529, loss: 0.004152366891503334\n","step: 530, loss: 0.05164165049791336\n","step: 531, loss: 0.012830867432057858\n","step: 532, loss: 0.03003818728029728\n","step: 533, loss: 0.03323955833911896\n","step: 534, loss: 0.01890057884156704\n","step: 535, loss: 0.13445180654525757\n","step: 536, loss: 0.1474963277578354\n","step: 537, loss: 0.010227352380752563\n","step: 538, loss: 0.013397688046097755\n","step: 539, loss: 0.03239801898598671\n","step: 540, loss: 0.0038116485811769962\n","step: 541, loss: 0.002519044326618314\n","step: 542, loss: 0.004099145065993071\n","step: 543, loss: 0.00543157197535038\n","step: 544, loss: 0.021611018106341362\n","step: 545, loss: 0.035382214933633804\n","step: 546, loss: 0.005474741570651531\n","step: 547, loss: 0.015609889291226864\n","step: 548, loss: 0.006589372642338276\n","step: 549, loss: 0.21302516758441925\n","step: 550, loss: 0.0025279917754232883\n","step: 551, loss: 0.0022261952981352806\n","step: 552, loss: 0.009652300737798214\n","step: 553, loss: 0.020903078839182854\n","step: 554, loss: 0.02139212377369404\n","step: 555, loss: 0.007835508324205875\n","step: 556, loss: 0.03041425161063671\n","step: 557, loss: 0.06010492891073227\n","step: 558, loss: 0.02209632098674774\n","step: 559, loss: 0.005755169782787561\n","step: 560, loss: 0.04152490198612213\n","step: 561, loss: 0.018722010776400566\n","step: 562, loss: 0.009824907407164574\n","step: 563, loss: 0.051612842828035355\n","step: 564, loss: 0.0058976649306714535\n","step: 565, loss: 0.03515748679637909\n","step: 566, loss: 0.020651094615459442\n","step: 567, loss: 0.044300008565187454\n","step: 568, loss: 0.01142624020576477\n","step: 569, loss: 0.01134387869387865\n","step: 570, loss: 0.006498485803604126\n","step: 571, loss: 0.02261395752429962\n","step: 572, loss: 0.004459727089852095\n","step: 573, loss: 0.1160031333565712\n","step: 574, loss: 0.07244820147752762\n","step: 575, loss: 0.019459262490272522\n","step: 576, loss: 0.0020814656745642424\n","step: 577, loss: 0.035873278975486755\n","step: 578, loss: 0.020854683592915535\n","step: 579, loss: 0.007683888543397188\n","step: 580, loss: 0.007408143486827612\n","step: 581, loss: 0.05483068898320198\n","step: 582, loss: 0.004458435345441103\n","step: 583, loss: 0.06651858985424042\n","step: 584, loss: 0.006540673319250345\n","step: 585, loss: 0.19268201291561127\n","step: 586, loss: 0.007009453605860472\n","step: 587, loss: 0.0027562251780182123\n","step: 588, loss: 0.19686216115951538\n","step: 589, loss: 0.007095173001289368\n","step: 590, loss: 0.004497718997299671\n","step: 591, loss: 0.11166366934776306\n","step: 592, loss: 0.0017206721240654588\n","step: 593, loss: 0.03298437222838402\n","step: 594, loss: 0.020601028576493263\n","step: 595, loss: 0.002012897515669465\n","step: 596, loss: 0.010484919883310795\n","step: 597, loss: 0.015741532668471336\n","step: 598, loss: 0.02302543632686138\n","step: 599, loss: 0.019776294007897377\n","step: 600, loss: 0.008899105712771416\n","step: 601, loss: 0.013253400102257729\n","step: 602, loss: 0.012612784281373024\n","step: 603, loss: 0.0017655700212344527\n","step: 604, loss: 0.002966751344501972\n","step: 605, loss: 0.0024599379394203424\n","step: 606, loss: 0.06768247485160828\n","step: 607, loss: 0.0023462618701159954\n","step: 608, loss: 0.00208491925150156\n","step: 609, loss: 0.0013543606037274003\n","step: 610, loss: 0.003806872060522437\n","step: 611, loss: 0.004352320916950703\n","step: 612, loss: 0.003152217948809266\n","step: 613, loss: 0.0036742559168487787\n","step: 614, loss: 0.0020662196911871433\n","step: 615, loss: 0.0018685795366764069\n","step: 616, loss: 0.0014116610400378704\n","step: 617, loss: 0.0017925319261848927\n","step: 618, loss: 0.08464446663856506\n","step: 619, loss: 0.041942302137613297\n","step: 620, loss: 0.0024605216458439827\n","step: 621, loss: 0.001691467477940023\n","step: 622, loss: 0.0038433552253991365\n","step: 623, loss: 0.0023337560705840588\n","step: 624, loss: 0.0017478871159255505\n","step: 625, loss: 0.02372175268828869\n","step: 626, loss: 0.0034548426046967506\n","step: 627, loss: 0.004066845402121544\n","step: 628, loss: 0.0031399810686707497\n","step: 629, loss: 0.001496136886999011\n","step: 630, loss: 0.01507139578461647\n","step: 631, loss: 0.02193537726998329\n","step: 632, loss: 0.03271478787064552\n","step: 633, loss: 0.0010577809298411012\n","step: 634, loss: 0.0015843968139961362\n","step: 635, loss: 0.0234487634152174\n","step: 636, loss: 0.0019181584939360619\n","step: 637, loss: 0.0011682059848681092\n","step: 638, loss: 0.002721620723605156\n","step: 639, loss: 0.006818191614001989\n","step: 640, loss: 0.002041557105258107\n","step: 641, loss: 0.011160743422806263\n","step: 642, loss: 0.002371416660025716\n","step: 643, loss: 0.003028536681085825\n","step: 644, loss: 0.003442913992330432\n","step: 645, loss: 0.0024641300551593304\n","step: 646, loss: 0.006861379370093346\n","step: 647, loss: 0.0046264128759503365\n","step: 648, loss: 0.019294477999210358\n","step: 649, loss: 0.019687915220856667\n","step: 650, loss: 0.002187431091442704\n","step: 651, loss: 0.09027014672756195\n","step: 652, loss: 0.0009675528854131699\n","step: 653, loss: 0.0040175653994083405\n","step: 654, loss: 0.001971457153558731\n","step: 655, loss: 0.001725133042782545\n","step: 656, loss: 0.0044790166430175304\n","step: 657, loss: 0.006145864259451628\n","step: 658, loss: 0.06506573408842087\n","step: 659, loss: 0.004134783521294594\n","step: 660, loss: 0.02473372407257557\n","step: 661, loss: 0.0030209484975785017\n","step: 662, loss: 0.003438378684222698\n","step: 663, loss: 0.00275146821513772\n","step: 664, loss: 0.08630550652742386\n","step: 665, loss: 0.005124255549162626\n","step: 666, loss: 0.0008234286797232926\n","step: 667, loss: 0.003469875082373619\n","step: 668, loss: 0.005880123004317284\n","step: 669, loss: 0.006445817183703184\n","step: 670, loss: 0.0025438570883125067\n","step: 671, loss: 0.012165798805654049\n","step: 672, loss: 0.0042326380498707294\n","step: 673, loss: 0.0007966739940457046\n","step: 674, loss: 0.21445907652378082\n","step: 675, loss: 0.0024816985242068768\n","step: 676, loss: 0.0023487170692533255\n","step: 677, loss: 0.0018227893160656095\n","step: 678, loss: 0.0017761584604158998\n","step: 679, loss: 0.0016667625168338418\n","step: 680, loss: 0.09819121658802032\n","step: 681, loss: 0.003338893875479698\n","step: 682, loss: 0.0019276771927252412\n","step: 683, loss: 0.040527474135160446\n","step: 684, loss: 0.0034248828887939453\n","step: 685, loss: 0.0032164687290787697\n","step: 686, loss: 0.0021583314519375563\n","step: 687, loss: 0.0013293324736878276\n","step: 688, loss: 0.005826106760650873\n","step: 689, loss: 0.0028282294515520334\n","step: 690, loss: 0.0010861002374440432\n","step: 691, loss: 0.0020548186730593443\n","step: 692, loss: 0.0017650434747338295\n","step: 693, loss: 0.002063922816887498\n","step: 694, loss: 0.0017371529247611761\n","step: 695, loss: 0.001415107399225235\n","step: 696, loss: 0.020106615498661995\n","step: 697, loss: 0.0019960321951657534\n","step: 698, loss: 0.12893296778202057\n","step: 699, loss: 0.011797875165939331\n","step: 700, loss: 0.001622291631065309\n","step: 701, loss: 0.0030640331096947193\n","step: 702, loss: 0.008189525455236435\n","step: 703, loss: 0.0026484462432563305\n","step: 704, loss: 0.010339218191802502\n","step: 705, loss: 0.0014580718707293272\n","step: 706, loss: 0.0016704426379874349\n","step: 707, loss: 0.0011501513654366136\n","step: 708, loss: 0.001966071082279086\n","step: 709, loss: 0.0025449893437325954\n","step: 710, loss: 0.001218015095219016\n","step: 711, loss: 0.002672030823305249\n","step: 712, loss: 0.0008512258646078408\n","step: 713, loss: 0.0012280631344765425\n","step: 714, loss: 0.001318324008025229\n","step: 715, loss: 0.027726318687200546\n","step: 716, loss: 0.005130857694894075\n","step: 717, loss: 0.02461915649473667\n","step: 718, loss: 0.0023539087269455194\n","step: 719, loss: 0.0009204449015669525\n","step: 720, loss: 0.004831300582736731\n","step: 721, loss: 0.0009501943713985384\n","step: 722, loss: 0.03584972023963928\n","step: 723, loss: 0.0023299369495362043\n","step: 724, loss: 0.0043340325355529785\n","step: 725, loss: 0.0062049138359725475\n","step: 726, loss: 0.001930351834744215\n","step: 727, loss: 0.0007980508380569518\n","step: 728, loss: 0.002842668676748872\n","step: 729, loss: 0.015236717648804188\n","step: 730, loss: 0.0017669110093265772\n","step: 731, loss: 0.0030633092392235994\n","step: 732, loss: 0.002821236150339246\n","step: 733, loss: 0.0022121532820165157\n","step: 734, loss: 0.00433648657053709\n","step: 735, loss: 0.002503118244931102\n","step: 736, loss: 0.003855430753901601\n","step: 737, loss: 0.011529212817549706\n","step: 738, loss: 0.0012637354666367173\n","step: 739, loss: 0.017437748610973358\n","step: 740, loss: 0.0016901397611945868\n","step: 741, loss: 0.046625394374132156\n","step: 742, loss: 0.0013372268294915557\n","step: 743, loss: 0.0034739121329039335\n","step: 744, loss: 0.0018098946893587708\n","step: 745, loss: 0.002639895072206855\n","step: 746, loss: 0.0019862325862050056\n","step: 747, loss: 0.002577573759481311\n","step: 748, loss: 0.0016349616926163435\n","step: 749, loss: 0.00410812022164464\n","step: 750, loss: 0.015760086476802826\n","step: 751, loss: 0.00942310132086277\n","step: 752, loss: 0.0014967006864026189\n","step: 753, loss: 0.00402472261339426\n","step: 754, loss: 0.0020119561813771725\n","step: 755, loss: 0.002150885295122862\n","step: 756, loss: 0.0017862424720078707\n","step: 757, loss: 0.0007528444984927773\n","step: 758, loss: 0.0009830920025706291\n","step: 759, loss: 0.0011340073542669415\n","step: 760, loss: 0.0013952547451481223\n","step: 761, loss: 0.0012236960465088487\n","step: 762, loss: 0.0007226455491036177\n","step: 763, loss: 0.000885156390722841\n","step: 764, loss: 0.0010958476923406124\n","step: 765, loss: 0.07525377720594406\n","step: 766, loss: 0.0013925823150202632\n","step: 767, loss: 0.0010531150037422776\n","step: 768, loss: 0.0011005420237779617\n","step: 769, loss: 0.0031329370103776455\n","step: 770, loss: 0.0012354860082268715\n","step: 771, loss: 0.015993040055036545\n","step: 772, loss: 0.0009502042667008936\n","step: 773, loss: 0.0016058114124462008\n","step: 774, loss: 0.0025691098999232054\n","step: 775, loss: 0.0013602867256850004\n","step: 776, loss: 0.0031117843464016914\n","step: 777, loss: 0.0009211371652781963\n","step: 778, loss: 0.0006321030086837709\n","step: 779, loss: 0.0008940061670728028\n","step: 780, loss: 0.0017275004647672176\n","step: 781, loss: 0.0005289744585752487\n","step: 782, loss: 0.0013237148523330688\n","step: 783, loss: 0.0032256576232612133\n","step: 784, loss: 0.004349860362708569\n","step: 785, loss: 0.0018824896542355418\n","step: 786, loss: 0.00111748359631747\n","step: 787, loss: 0.003188850823789835\n","step: 788, loss: 0.0021126875653862953\n","step: 789, loss: 0.0017474443884566426\n","step: 790, loss: 0.0011130140628665686\n","step: 791, loss: 0.06541695445775986\n","step: 792, loss: 0.001295536756515503\n","step: 793, loss: 0.004585521295666695\n","step: 794, loss: 0.001179256709292531\n","step: 795, loss: 0.007113209459930658\n","step: 796, loss: 0.12616148591041565\n","step: 797, loss: 0.00154358078725636\n","step: 798, loss: 0.001159612089395523\n","step: 799, loss: 0.05032338574528694\n","step: 800, loss: 0.002862747525796294\n","step: 801, loss: 0.0006040724110789597\n","step: 802, loss: 0.0015609986148774624\n","step: 803, loss: 0.0007713264203630388\n","step: 804, loss: 0.0013423621421679854\n","step: 805, loss: 0.0017603104934096336\n","step: 806, loss: 0.0011183730093762279\n","step: 807, loss: 0.002429170534014702\n","step: 808, loss: 0.0010538797359913588\n","step: 809, loss: 0.0013599394587799907\n","step: 810, loss: 0.0024403405841439962\n","step: 811, loss: 0.002117851749062538\n","step: 812, loss: 0.0016467608511447906\n","step: 813, loss: 0.02349327877163887\n","step: 814, loss: 0.001055427361279726\n","step: 815, loss: 0.003001994453370571\n","step: 816, loss: 0.0015697431517764926\n","step: 817, loss: 0.0006577842868864536\n","step: 818, loss: 0.0010368318762630224\n","step: 819, loss: 0.058150142431259155\n","step: 820, loss: 0.003879173891618848\n","step: 821, loss: 0.0015230018179863691\n","step: 822, loss: 0.0009516273275949061\n","step: 823, loss: 0.0018275644397363067\n","step: 824, loss: 0.004899430554360151\n","step: 825, loss: 0.10567617416381836\n","step: 826, loss: 0.0011129295453429222\n","step: 827, loss: 0.001252258662134409\n","step: 828, loss: 0.0005516244564205408\n","step: 829, loss: 0.0014376133913174272\n","step: 830, loss: 0.0019087748369202018\n","step: 831, loss: 0.000748762337025255\n","step: 832, loss: 0.000903594889678061\n","step: 833, loss: 0.0019218918168917298\n","step: 834, loss: 0.0016257078386843204\n","step: 835, loss: 0.001550579909235239\n","step: 836, loss: 0.0016103818779811263\n","step: 837, loss: 0.001707613468170166\n","step: 838, loss: 0.0012904652394354343\n","step: 839, loss: 0.0008382281521335244\n","step: 840, loss: 0.0006129134562797844\n","step: 841, loss: 0.0017820775974541903\n","step: 842, loss: 0.0008018722874112427\n","step: 843, loss: 0.0006807353347539902\n","step: 844, loss: 0.002429857151582837\n","step: 845, loss: 0.02687089890241623\n","step: 846, loss: 0.0010146715212613344\n","step: 847, loss: 0.001564627862535417\n","step: 848, loss: 0.0005999404820613563\n","step: 849, loss: 0.002075941301882267\n","step: 850, loss: 0.0009411537903361022\n","step: 851, loss: 0.0005579237476922572\n","step: 852, loss: 0.0018796134972944856\n","step: 853, loss: 0.0010986423585563898\n","step: 854, loss: 0.003785784589126706\n","step: 855, loss: 0.0011376711772754788\n","step: 856, loss: 0.0013667732710018754\n","step: 857, loss: 0.002077041892334819\n","step: 858, loss: 0.0012415630044415593\n","step: 859, loss: 0.004054325167089701\n","step: 860, loss: 0.0014500347897410393\n","step: 861, loss: 0.0031474214047193527\n","step: 862, loss: 0.0008654423872940242\n","step: 863, loss: 0.0022971213329583406\n","step: 864, loss: 0.0019717186223715544\n","step: 865, loss: 0.0003465261252131313\n","step: 866, loss: 0.0013387036742642522\n","step: 867, loss: 0.0009393510408699512\n","step: 868, loss: 0.0007994399056769907\n","step: 869, loss: 0.0005780766368843615\n","step: 870, loss: 0.0011191728990525007\n","step: 871, loss: 0.0015666574472561479\n","step: 872, loss: 0.002345085609704256\n","step: 873, loss: 0.0016787126660346985\n","step: 874, loss: 0.0008343599620275199\n","step: 875, loss: 0.008068051189184189\n","step: 876, loss: 0.0023198642302304506\n","step: 877, loss: 0.002072613686323166\n","step: 878, loss: 0.0014183286111801863\n","step: 879, loss: 0.04246539995074272\n","step: 880, loss: 0.0008449548622593284\n","step: 881, loss: 0.0026548432651907206\n","step: 882, loss: 0.0009499360457994044\n","step: 883, loss: 0.0009378652321174741\n","step: 884, loss: 0.0012923707254230976\n","step: 885, loss: 0.0018982338951900601\n","step: 886, loss: 0.0011232072720304132\n","step: 887, loss: 0.0007139116642065346\n","step: 888, loss: 0.002196224872022867\n","step: 889, loss: 0.001215622411109507\n","step: 890, loss: 0.0011055697686970234\n","step: 891, loss: 0.0008006636635400355\n","step: 892, loss: 0.0011438577203080058\n","step: 893, loss: 0.07157548516988754\n","step: 894, loss: 0.0011879585217684507\n","step: 895, loss: 0.0003945745120290667\n","step: 896, loss: 0.0009983384516090155\n","step: 897, loss: 0.05235585197806358\n","step: 898, loss: 0.000700388103723526\n","step: 899, loss: 0.06259483844041824\n","step: 900, loss: 0.0006986470543779433\n","step: 901, loss: 0.0004257354012224823\n","step: 902, loss: 0.00048016986693255603\n","step: 903, loss: 0.000774400366935879\n","step: 904, loss: 0.0010092387674376369\n","step: 905, loss: 0.08559329062700272\n","step: 906, loss: 0.0010883101494982839\n","step: 907, loss: 0.001286758342757821\n","step: 908, loss: 0.000802996102720499\n","step: 909, loss: 0.0013493832666426897\n","step: 910, loss: 0.001397940912283957\n","step: 911, loss: 0.0010421837214380503\n","step: 912, loss: 0.0014691312098875642\n","step: 913, loss: 0.0017711459659039974\n","step: 914, loss: 0.0011543040163815022\n","step: 915, loss: 0.0009440438589081168\n","step: 916, loss: 0.0016819963930174708\n","step: 917, loss: 0.0019424638012424111\n","step: 918, loss: 0.0004721802833955735\n","step: 919, loss: 0.001892805565148592\n","step: 920, loss: 0.043473970144987106\n","step: 921, loss: 0.0012872410006821156\n","step: 922, loss: 0.0005725747905671597\n","step: 923, loss: 0.000584100722335279\n","step: 924, loss: 0.05605094134807587\n","step: 925, loss: 0.002033642726019025\n","step: 926, loss: 0.0018970902310684323\n","step: 927, loss: 0.0010888956021517515\n","step: 928, loss: 0.0056723617017269135\n","step: 929, loss: 0.0014480368699878454\n","step: 930, loss: 0.015113933011889458\n","step: 931, loss: 0.0008179081487469375\n","step: 932, loss: 0.0010202405974268913\n","step: 933, loss: 0.0021911747753620148\n","step: 934, loss: 0.0015100868185982108\n","step: 935, loss: 0.0008501987322233617\n","step: 936, loss: 0.0011216090060770512\n","step: 937, loss: 0.0015669033164158463\n","step: 938, loss: 0.0015650932909920812\n","step: 939, loss: 0.0015287683345377445\n","step: 940, loss: 0.0012181713245809078\n","step: 941, loss: 0.0011119348928332329\n","step: 942, loss: 0.0022565186955034733\n","step: 943, loss: 0.0008434237679466605\n","step: 944, loss: 0.0006299940287135541\n","step: 945, loss: 0.0003967154480051249\n","step: 946, loss: 0.0017770263366401196\n","step: 947, loss: 0.001309343264438212\n","step: 948, loss: 0.00427792314440012\n","step: 949, loss: 0.0011906373547390103\n","step: 950, loss: 0.0015913941897451878\n","step: 951, loss: 0.0008367520640604198\n","step: 952, loss: 0.000753262487705797\n","step: 953, loss: 0.0007598859956488013\n","step: 954, loss: 0.0008786282851360738\n","step: 955, loss: 0.0036455178633332253\n","step: 956, loss: 0.0009113153791986406\n","step: 957, loss: 0.0006287902360782027\n","step: 958, loss: 0.0007717203116044402\n","step: 959, loss: 0.0010894641745835543\n","step: 960, loss: 0.0004594969504978508\n","step: 961, loss: 0.0006831457722000778\n","step: 962, loss: 0.00028744564042426646\n","step: 963, loss: 0.0007925779209472239\n","step: 964, loss: 0.0007441152702085674\n","step: 965, loss: 0.0016759634017944336\n","step: 966, loss: 0.002406206913292408\n","step: 967, loss: 0.0007142055546864867\n","step: 968, loss: 0.0008175794500857592\n","step: 969, loss: 0.0005229588714428246\n","step: 970, loss: 0.0008771273423917592\n","step: 971, loss: 0.0007143006660044193\n","step: 972, loss: 0.0013457764871418476\n","step: 973, loss: 0.0006387431640177965\n","step: 974, loss: 0.0007399642490781844\n","step: 975, loss: 0.00045032970956526697\n","step: 976, loss: 0.0013733097584918141\n","step: 977, loss: 0.0018036904511973262\n","step: 978, loss: 0.000806196650955826\n","step: 979, loss: 0.001919468049891293\n","step: 980, loss: 0.0007728010532446206\n","step: 981, loss: 0.00048462243285030127\n","step: 982, loss: 0.014801310375332832\n","step: 983, loss: 0.0753680169582367\n","step: 984, loss: 0.0007998326327651739\n","step: 985, loss: 0.0011580548016354442\n","step: 986, loss: 0.000804802228230983\n","step: 987, loss: 0.0037831654772162437\n","step: 988, loss: 0.0007154896738938987\n","step: 989, loss: 0.0010982665698975325\n","step: 990, loss: 0.0009181880741380155\n","step: 991, loss: 0.0008245001081377268\n","step: 992, loss: 0.04348547384142876\n","step: 993, loss: 0.0016637773951515555\n","step: 994, loss: 0.001341438852250576\n","step: 995, loss: 0.0008311893907375634\n","step: 996, loss: 0.0012383649591356516\n","step: 997, loss: 0.0008509898325428367\n","step: 998, loss: 0.0007165704155340791\n","step: 999, loss: 0.0007052155560813844\n","step: 1000, loss: 0.0014744031941518188\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyhX5bxTKlT0","executionInfo":{"status":"ok","timestamp":1621005773822,"user_tz":-420,"elapsed":2318,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"}},"outputId":"a24de08c-20b3-4aad-d0d3-c90a1f7d5887"},"source":["# test\n","\n","with tf.Session() as sess:\n","  epoch = train_data_reader._num_epoch\n","\n","  trainable_variables = tf.trainable_variables()\n","  for variable in trainable_variables:\n","    saved_value = restore_parameters(variable.name, epoch)\n","    assign_op = variable.assign(saved_value)\n","    sess.run(assign_op)\n","\n","  num_true_preds = 0\n","  while True:\n","    test_data, test_labels = test_data_reader.next_batch()\n","    test_plabels_eval = sess.run(\n","        predicted_labels,\n","        feed_dict={\n","            mlp._X: test_data,\n","            mlp._real_Y: test_labels\n","        }\n","    )\n","    matches = np.equal(test_plabels_eval, test_labels)\n","    num_true_preds += np.sum(matches.astype(float))\n","\n","    if test_data_reader._batch_id == 0:\n","      break\n","      \n","  print('Epoch:{}'.format(epoch))\n","  print('Accuracy on test data:{}'.format(num_true_preds / len(test_data_reader._data)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch:6\n","Accuracy on test data:0.7761180837899947\n"],"name":"stdout"}]}]}